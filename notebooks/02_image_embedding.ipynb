{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3580d87e",
   "metadata": {},
   "source": [
    "## Step 2: Image Embedding with CLIP\n",
    "This notebook covers the process of generating image embeddings using the CLIP model. We will load images, preprocess them, and extract embeddings for downstream tasks such as similarity search and clustering.\n",
    "\n",
    "- Load image paths and metadata\n",
    "- Preprocess images for CLIP\n",
    "- Generate and save image embeddings\n",
    "- Discuss best practices and troubleshooting tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b004301a",
   "metadata": {},
   "source": [
    "## CLIP Model Setup & Image Selection\n",
    "Import CLIP model, set up device, and define helper functions to select the best image URL from metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6de2a977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import io, time, math, requests, sys, os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
    "from src.data_utils import preprocess_df\n",
    "from src.clip_processing import pick_best_image_from_images_field\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device).eval()\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "df_review = pd.read_parquet(\"../data/CDs_and_Vinyl_reviews.parquet\")\n",
    "df_meta = pd.read_parquet(\"../data/CDs_and_Vinyl_meta.parquet\")\n",
    "df = preprocess_df(df_review, df_meta)\n",
    "\n",
    "df['img_url'] = df['images_y'].apply(pick_best_image_from_images_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5077f34",
   "metadata": {},
   "source": [
    "## Image Downloading, Embedding, and Saving\n",
    "Download images, convert to PIL, embed with CLIP, and save embeddings in chunks. Merge all parts and update the main DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe51ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io, gc, requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
    "from src.clip_processing import get_session, fetch_image_bytes, to_pil, embed_pil_batch\n",
    "\n",
    "# ================================================================\n",
    "# CONFIGURATION\n",
    "# ================================================================\n",
    "SAMPLE = 50000\n",
    "CHUNK = 5000\n",
    "N_WORKERS = 24\n",
    "CLIP_BATCH = 32\n",
    "IMG_CACHE = \"../img_cache\"\n",
    "SAVE_DIR = \"../emb_parts\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(IMG_CACHE, exist_ok=True)\n",
    "CATEGORY = \"CDs_and_Vinyl\"\n",
    "\n",
    "# ================================================================\n",
    "# REQUEST SESSION\n",
    "# ================================================================\n",
    "SESSION = get_session()\n",
    "\n",
    "# ================================================================\n",
    "# PREPARE DATAFRAME SUBSET\n",
    "# ================================================================\n",
    "df = pd.read_parquet(f\"../data_{CATEGORY}_prepared.parquet\")\n",
    "mask = df[\"img_url\"].notna()\n",
    "work = df.loc[mask, [\"parent_asin\", \"img_url\"]].drop_duplicates(\"parent_asin\").copy()\n",
    "if len(work) > SAMPLE:\n",
    "    work = work.sample(SAMPLE, random_state=42).reset_index(drop=True)\n",
    "print(\"Total images to process:\", len(work))\n",
    "\n",
    "# ================================================================\n",
    "# MAIN PROCESS LOOP\n",
    "# ================================================================\n",
    "def part_path(pi):\n",
    "    return os.path.join(SAVE_DIR, f\"clip_img_emb_parent.part{pi:03d}.parquet\")\n",
    "\n",
    "num_parts = (len(work) + CHUNK - 1) // CHUNK\n",
    "\n",
    "for pi in range(num_parts):\n",
    "    out_path = part_path(pi)\n",
    "    if os.path.exists(out_path):\n",
    "        print(f\"[SKIP] Part {pi} exists -> {out_path}\")\n",
    "        continue\n",
    "\n",
    "    start, stop = pi * CHUNK, min(len(work), (pi + 1) * CHUNK)\n",
    "    sub = work.iloc[start:stop].reset_index(drop=True)\n",
    "    print(f\"\\n[Part {pi}] rows {start}:{stop} ({len(sub)})\")\n",
    "\n",
    "    # ---------- Download ----------\n",
    "    bytes_list = [None] * len(sub)\n",
    "    with ThreadPoolExecutor(max_workers=N_WORKERS) as ex:\n",
    "        futs = {ex.submit(fetch_image_bytes, url, session=SESSION, fname=sub.loc[i, \"parent_asin\"], cache_dir=IMG_CACHE): i\n",
    "                for i, url in enumerate(sub[\"img_url\"])}\n",
    "        for fut in tqdm(as_completed(futs), total=len(futs), desc=f\"Downloading P{pi}\"):\n",
    "            i = futs[fut]\n",
    "            bytes_list[i] = fut.result()\n",
    "\n",
    "    # ---------- Convert to PIL ----------\n",
    "    pil_list = [to_pil(b) for b in bytes_list]\n",
    "    ok_rate = np.mean([p is not None for p in pil_list])\n",
    "    print(f\"[Part {pi}] PIL OK rate: {ok_rate:.1%}\")\n",
    "\n",
    "    # ---------- Embed ----------\n",
    "    embs = embed_pil_batch(pil_list, processor, model, device, batch_size=CLIP_BATCH)\n",
    "\n",
    "    # ---------- Save partial parquet ----------\n",
    "    rows = [(str(sub.loc[i, \"parent_asin\"]), e.tolist()) for i, e in enumerate(embs) if e is not None]\n",
    "    part_df = pd.DataFrame(rows, columns=[\"parent_asin\", \"clip_img_emb\"])\n",
    "    part_df.to_parquet(out_path, index=False, compression=\"zstd\")\n",
    "    print(f\"[Part {pi}] Saved {len(part_df)} embeddings -> {out_path}\")\n",
    "\n",
    "    # ---------- Cleanup ----------\n",
    "    del bytes_list, pil_list, embs, part_df, sub\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nâœ… All parts done (or skipped).\")\n",
    "\n",
    "# ================================================================\n",
    "# MERGE ALL PARTS\n",
    "# ================================================================\n",
    "parts = [part_path(pi) for pi in range(num_parts) if os.path.exists(part_path(pi))]\n",
    "if parts:\n",
    "    merged_emb = pd.concat([pd.read_parquet(p) for p in parts], ignore_index=True)\n",
    "    merged_emb = merged_emb.drop_duplicates(\"parent_asin\")\n",
    "    merged_emb.to_parquet(\"../clip_img_emb_parent.parquet\", index=False, compression=\"zstd\")\n",
    "    print(\"Final merged embeddings:\", len(merged_emb))\n",
    "\n",
    "    # ================================================================\n",
    "    # MERGE BACK TO MAIN DF\n",
    "    # ================================================================\n",
    "    df[\"parent_asin\"] = df[\"parent_asin\"].astype(str)\n",
    "    merged_emb[\"parent_asin\"] = merged_emb[\"parent_asin\"].astype(str)\n",
    "    df = df.drop(columns=[\"clip_img_emb\"], errors=\"ignore\").merge(merged_emb, on=\"parent_asin\", how=\"left\")\n",
    "\n",
    "    df[\"has_img_emb\"] = df[\"clip_img_emb\"].notna()\n",
    "    df[\"clip_img_emb\"] = df[\"clip_img_emb\"].apply(\n",
    "        lambda v: np.array(v, dtype=np.float32) if isinstance(v, list) else np.zeros(model.config.projection_dim, dtype=np.float32)\n",
    "    )\n",
    "\n",
    "    df.to_parquet(f\"../reviews_with_img_emb_{CATEGORY}.parquet\", index=False, engine=\"pyarrow\", compression=\"zstd\")\n",
    "    print(\" Saved final df:\", f\"../reviews_with_img_emb_{CATEGORY}.parquet\")\n",
    "else:\n",
    "    print(\"No embedding parts were created.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
